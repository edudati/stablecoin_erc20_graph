{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cba6d9b",
   "metadata": {},
   "source": [
    "This notebook constructs daily directed transaction graphs for each stablecoin based on cleaned ERC20 transfer data. Each node represents a wallet address, and each directed edge represents a token transfer between addresses, weighted by the transferred amount.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a939d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "from datetime import datetime\n",
    "\n",
    "input_file = '../data/cleaned/token_transfers_V3.0.0_cleaned.parquet'\n",
    "output_dir = '../data/graphs/daily/'\n",
    "metrics_dir = '../data/graphs/metrics'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "os.makedirs(metrics_dir, exist_ok=True)\n",
    "\n",
    "df = pd.read_parquet(input_file)\n",
    "\n",
    "df.info()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24810df4",
   "metadata": {},
   "source": [
    "Iteration over each stablecoin and daily timestamp in the dataset to generate directed weighted transaction graphs. Each graph represents token transfers between wallets on a specific day and is saved as a .gpickle file for later network analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5304c516",
   "metadata": {},
   "outputs": [],
   "source": [
    "for stablecoin in df['stablecoin'].unique():\n",
    "    df_stablecoin = df[df['stablecoin'] == stablecoin]\n",
    "    \n",
    "    for date in df_stablecoin['date'].unique():\n",
    "        df_day = df_stablecoin[df_stablecoin['date'] == date]\n",
    "        \n",
    "        G = nx.DiGraph()\n",
    "        \n",
    "        for _, row in df_day.iterrows():\n",
    "            u = row['from_address']\n",
    "            v = row['to_address']\n",
    "            w = row['value']\n",
    "            if G.has_edge(u, v):\n",
    "                G[u][v]['weight'] += w\n",
    "            else:\n",
    "                G.add_edge(u, v, weight=w)\n",
    "        \n",
    "        # Save graph with Pickle\n",
    "        filename = f\"{stablecoin}_{date}.gpickle\"\n",
    "        path = os.path.join(output_dir, filename)\n",
    "        with open(path, 'wb') as f:\n",
    "            pickle.dump(G, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95076f65",
   "metadata": {},
   "source": [
    "This script loads each daily transaction graph per stablecoin, computes node-level metrics such as in-degree, out-degree, PageRank, and degree centrality, and stores the results in Parquet format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83cfd6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List .gpickle files\n",
    "gpickle_files = [f for f in os.listdir(output_dir) if f.endswith('.gpickle')]\n",
    "\n",
    "# Group files per stablecoin\n",
    "coin_files = {}\n",
    "for file in gpickle_files:\n",
    "    if '_' in file:\n",
    "        coin = file.split('_')[0]\n",
    "        coin_files.setdefault(coin, []).append(file)  # fix: use 'coin' not 'stablecoin'\n",
    "\n",
    "# Loop per stablecoin\n",
    "for coin, files in coin_files.items():\n",
    "    records = []\n",
    "\n",
    "    for file in tqdm(files, desc=f'Processing {coin}'):\n",
    "        path = os.path.join(output_dir, file)\n",
    "\n",
    "        # Load Graph\n",
    "        with open(path, 'rb') as f:\n",
    "            G = pickle.load(f)\n",
    "\n",
    "        date = file.replace(f'{coin}_', '').replace('.gpickle', '')\n",
    "\n",
    "        # Metrics\n",
    "        in_degrees = dict(G.in_degree())\n",
    "        out_degrees = dict(G.out_degree())\n",
    "        pagerank = nx.pagerank(G, alpha=0.85)\n",
    "        degree_centrality = nx.degree_centrality(G)\n",
    "\n",
    "        for node in G.nodes():\n",
    "            records.append({\n",
    "                'date': date,\n",
    "                'token': coin,\n",
    "                'address': node,\n",
    "                'in_degree': in_degrees.get(node, 0),\n",
    "                'out_degree': out_degrees.get(node, 0),\n",
    "                'pagerank': pagerank.get(node, 0),\n",
    "                'degree_centrality': degree_centrality.get(node, 0)\n",
    "            })\n",
    "\n",
    "    # Save metrics as Parquet\n",
    "    df_metrics = pd.DataFrame(records)\n",
    "    output_file = os.path.join(metrics_dir, f'{coin}_metrics.parquet')\n",
    "    df_metrics.to_parquet(output_file, index=False)\n",
    "    print(f'Saved: {output_file}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe560ed3",
   "metadata": {},
   "source": [
    "Loads all token-specific metric files, merges them into a single DataFrame, and stores the combined result in Parquet format to facilitate global comparative analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607a523d",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_files = glob(os.path.join(metrics_dir, '*_metrics.parquet'))\n",
    "\n",
    "dfs = []\n",
    "\n",
    "for file in metric_files:\n",
    "    token = os.path.basename(file).split('_')[0]\n",
    "    df = pd.read_parquet(file)\n",
    "    dfs.append(df)\n",
    "\n",
    "df_all = pd.concat(dfs, ignore_index=True)\n",
    "df_all.to_parquet(os.path.join(metrics_dir, 'all_metrics.parquet'), index=False)\n",
    "print(f\"Saved\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
